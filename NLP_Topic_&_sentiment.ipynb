{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg9JN0NejlCL",
        "outputId": "37e01dc1-46a2-4e2e-a138-bc14fa242343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyyDNXFejsRs",
        "outputId": "55cc9a92-1fe3-4c63-a25b-1e97d2cd3fc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['test.csv', 'train data.json']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/ai-1904-dpl-302-m-topic-sentiment-classification.zip\"  \n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Kiểm tra các file đã giải nén\n",
        "os.listdir(extract_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnhIfbLSkCzT"
      },
      "source": [
        "Đọc file train_data.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vublvi9EjvrS",
        "outputId": "8076abc8-0641-4d42-b41e-874d46c2e07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số mẫu huấn luyện: 1684\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Đọc train data\n",
        "with open(\"/content/data/train data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Tách text, sentiment, topic\n",
        "texts, sentiments, topics = [], [], []\n",
        "for item in train_data:\n",
        "    text = item[\"data\"][\"text\"]\n",
        "    sent, topic_list = None, []\n",
        "\n",
        "    for ann in item[\"annotations\"]:\n",
        "        if ann[\"from_name\"] == \"sentiment\":\n",
        "            sent = ann[\"value\"][\"choices\"][0]\n",
        "        elif ann[\"from_name\"] == \"topic\":\n",
        "            topic_list.extend(ann[\"value\"][\"choices\"])\n",
        "\n",
        "    if sent:\n",
        "        texts.append(text)\n",
        "        sentiments.append(sent)\n",
        "        topics.append(topic_list)\n",
        "\n",
        "print(f\"Số mẫu huấn luyện: {len(texts)}\")\n",
        "\n",
        "# Đọc test\n",
        "test_df = pd.read_csv(\"/content/data/test.csv\")\n",
        "test_texts = test_df[\"text\"].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QFuuoXdkoAy"
      },
      "source": [
        " 2. Khởi tạo PhoBERT + Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwNTwaL_uyEF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmCP1ANOuzM3"
      },
      "source": [
        "3. Xử lý label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvR-ugWbu0yE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "\n",
        "# Cảm xúc: Tiêu cực, Trung tính, Tích cực\n",
        "le_sent = LabelEncoder()\n",
        "sent_labels = le_sent.fit_transform(sentiments)\n",
        "\n",
        "# Chủ đề: multi-label\n",
        "mlb_topic = MultiLabelBinarizer()\n",
        "topic_labels = mlb_topic.fit_transform(topics)\n",
        "topic_names = mlb_topic.classes_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6itltMi-u202"
      },
      "source": [
        "4. Dataset dùng chung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlUle1_Ru4QT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, is_multilabel=False, max_len=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_multilabel = is_multilabel\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(self.labels[idx], dtype=torch.float if self.is_multilabel else torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yXgmAysu56M"
      },
      "source": [
        " 5. Model Sentiment và Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMWolW4Ru7Pu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class PhoBERT_Classifier(nn.Module):\n",
        "    def __init__(self, num_labels, is_multilabel=False):\n",
        "        super().__init__()\n",
        "        self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.phobert.config.hidden_size, num_labels)\n",
        "        self.is_multilabel = is_multilabel\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_token = output.last_hidden_state[:, 0, :]\n",
        "        cls_token = self.dropout(cls_token)\n",
        "        logits = self.classifier(cls_token)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG0N4SVUvAd_"
      },
      "source": [
        "6. Huấn luyện mô hình sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlQsTjLIvCEh",
        "outputId": "df81591d-0e29-4b4a-a004-0fb9f0525fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sentiment Epoch 1] Loss: 0.9179 | Acc: 0.5552 | F1-macro: 0.3169 | F1-micro: 0.5552\n",
            "[Sentiment Epoch 2] Loss: 0.7392 | Acc: 0.6390 | F1-macro: 0.5836 | F1-micro: 0.6390\n",
            "[Sentiment Epoch 3] Loss: 0.5984 | Acc: 0.7328 | F1-macro: 0.6990 | F1-micro: 0.7328\n",
            "[Sentiment Epoch 4] Loss: 0.4347 | Acc: 0.8385 | F1-macro: 0.8219 | F1-micro: 0.8385\n",
            "[Sentiment Epoch 5] Loss: 0.3020 | Acc: 0.8842 | F1-macro: 0.8755 | F1-micro: 0.8842\n",
            "[Sentiment Epoch 6] Loss: 0.1908 | Acc: 0.9382 | F1-macro: 0.9342 | F1-micro: 0.9382\n",
            "[Sentiment Epoch 7] Loss: 0.1727 | Acc: 0.9424 | F1-macro: 0.9362 | F1-micro: 0.9424\n",
            "[Sentiment Epoch 8] Loss: 0.1003 | Acc: 0.9691 | F1-macro: 0.9656 | F1-micro: 0.9691\n",
            "[Sentiment Epoch 9] Loss: 0.0695 | Acc: 0.9786 | F1-macro: 0.9775 | F1-micro: 0.9786\n",
            "[Sentiment Epoch 10] Loss: 0.0629 | Acc: 0.9804 | F1-macro: 0.9781 | F1-micro: 0.9804\n",
            "[Sentiment Epoch 11] Loss: 0.0451 | Acc: 0.9893 | F1-macro: 0.9886 | F1-micro: 0.9893\n",
            "[Sentiment Epoch 12] Loss: 0.0599 | Acc: 0.9816 | F1-macro: 0.9813 | F1-micro: 0.9816\n",
            "[Sentiment Epoch 13] Loss: 0.0767 | Acc: 0.9768 | F1-macro: 0.9734 | F1-micro: 0.9768\n",
            "[Sentiment Epoch 14] Loss: 0.0457 | Acc: 0.9852 | F1-macro: 0.9851 | F1-micro: 0.9852\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9927    0.9891    0.9909       274\n",
            "           1     0.9874    0.9874    0.9874       953\n",
            "           2     0.9760    0.9781    0.9770       457\n",
            "\n",
            "    accuracy                         0.9852      1684\n",
            "   macro avg     0.9854    0.9849    0.9851      1684\n",
            "weighted avg     0.9852    0.9852    0.9852      1684\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "# Dataset và mô hình\n",
        "sent_dataset = TextDataset(texts, sent_labels, tokenizer)\n",
        "sent_loader = DataLoader(sent_dataset, batch_size=16, shuffle=True)\n",
        "model_sent = PhoBERT_Classifier(num_labels=3).to(\"cuda\")\n",
        "\n",
        "# Huấn luyện\n",
        "optimizer = torch.optim.AdamW(model_sent.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "best_loss, patience = float(\"inf\"), 0\n",
        "\n",
        "for epoch in range(20):\n",
        "    model_sent.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in sent_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        labels = batch[\"label\"].to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model_sent(input_ids, attention_mask)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Dự đoán class và gom nhãn thực tế để tính metrics\n",
        "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg = total_loss / len(sent_loader)\n",
        "    # Tính các chỉ số metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    f1_micro = f1_score(all_labels, all_preds, average=\"micro\")\n",
        "    print(f\"[Sentiment Epoch {epoch+1}] Loss: {avg:.4f} | Acc: {acc:.4f} | F1-macro: {f1_macro:.4f} | F1-micro: {f1_micro:.4f}\")\n",
        "\n",
        "    if avg < best_loss:\n",
        "        best_loss = avg\n",
        "        best_sent = copy.deepcopy(model_sent.state_dict())\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= 3: break\n",
        "\n",
        "model_sent.load_state_dict(best_sent)\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o64Tf1YvEI4"
      },
      "source": [
        "7. Huấn luyện mô hình topic (multi-label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3_aiv3GvFkH",
        "outputId": "ff23c8eb-02c3-47c7-e8ef-6db8ba0bbec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Topic Epoch 1] Loss: 0.3734 | Acc: 0.0024 | F1-macro: 0.0232 | F1-micro: 0.0271\n",
            "[Topic Epoch 2] Loss: 0.2904 | Acc: 0.0018 | F1-macro: 0.0048 | F1-micro: 0.0007\n",
            "[Topic Epoch 3] Loss: 0.2666 | Acc: 0.0018 | F1-macro: 0.0000 | F1-micro: 0.0000\n",
            "[Topic Epoch 4] Loss: 0.2507 | Acc: 0.0018 | F1-macro: 0.0000 | F1-micro: 0.0000\n",
            "[Topic Epoch 5] Loss: 0.2322 | Acc: 0.0160 | F1-macro: 0.0206 | F1-micro: 0.0392\n",
            "[Topic Epoch 6] Loss: 0.2102 | Acc: 0.0819 | F1-macro: 0.0952 | F1-micro: 0.2097\n",
            "[Topic Epoch 7] Loss: 0.1916 | Acc: 0.1574 | F1-macro: 0.1738 | F1-micro: 0.3628\n",
            "[Topic Epoch 8] Loss: 0.1750 | Acc: 0.2316 | F1-macro: 0.2374 | F1-micro: 0.4755\n",
            "[Topic Epoch 9] Loss: 0.1606 | Acc: 0.2892 | F1-macro: 0.3001 | F1-micro: 0.5614\n",
            "[Topic Epoch 10] Loss: 0.1477 | Acc: 0.3314 | F1-macro: 0.3636 | F1-micro: 0.6210\n",
            "[Topic Epoch 11] Loss: 0.1358 | Acc: 0.3961 | F1-macro: 0.4355 | F1-micro: 0.6783\n",
            "[Topic Epoch 12] Loss: 0.1238 | Acc: 0.4477 | F1-macro: 0.4780 | F1-micro: 0.7263\n",
            "[Topic Epoch 13] Loss: 0.1145 | Acc: 0.4976 | F1-macro: 0.5338 | F1-micro: 0.7630\n",
            "[Topic Epoch 14] Loss: 0.1052 | Acc: 0.5374 | F1-macro: 0.5683 | F1-micro: 0.7943\n",
            "[Topic Epoch 15] Loss: 0.0976 | Acc: 0.5790 | F1-macro: 0.6179 | F1-micro: 0.8197\n",
            "[Topic Epoch 16] Loss: 0.0892 | Acc: 0.6354 | F1-macro: 0.6668 | F1-micro: 0.8484\n",
            "[Topic Epoch 17] Loss: 0.0817 | Acc: 0.6698 | F1-macro: 0.7003 | F1-micro: 0.8685\n",
            "[Topic Epoch 18] Loss: 0.0751 | Acc: 0.7126 | F1-macro: 0.7216 | F1-micro: 0.8871\n",
            "[Topic Epoch 19] Loss: 0.0707 | Acc: 0.7292 | F1-macro: 0.7567 | F1-micro: 0.8997\n",
            "[Topic Epoch 20] Loss: 0.0651 | Acc: 0.7708 | F1-macro: 0.7985 | F1-micro: 0.9160\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "  Bất động sản     1.0000    0.8571    0.9231        70\n",
            "     Chính trị     1.0000    0.8358    0.9106        67\n",
            "   Chứng khoán     1.0000    0.8857    0.9394        35\n",
            "      Covid-19     1.0000    0.9272    0.9622       206\n",
            "     Công nghệ     0.9717    0.8655    0.9156       119\n",
            "       Du lịch     1.0000    0.5789    0.7333        38\n",
            "          Game     1.0000    0.3889    0.5600        18\n",
            "    Giao thông     0.9545    0.6364    0.7636        33\n",
            "      Giáo dục     1.0000    0.7708    0.8706        48\n",
            "      Giải trí     0.9820    0.9008    0.9397       242\n",
            "     Hóng biến     0.9939    0.9006    0.9449       181\n",
            "      Khoa học     0.0000    0.0000    0.0000        27\n",
            "Không xác định     0.0000    0.0000    0.0000        17\n",
            "       Kinh tế     0.9920    0.9151    0.9520       271\n",
            "    Môi trường     1.0000    0.8462    0.9167        65\n",
            "      Phim ảnh     1.0000    0.6154    0.7619        39\n",
            "     Pháp luật     1.0000    0.8349    0.9100       109\n",
            "      Sức khoẻ     1.0000    0.8613    0.9255       137\n",
            "      Thế giới     1.0000    0.9154    0.9558       201\n",
            "      Thể thao     0.9924    0.9559    0.9738       136\n",
            "       Văn hoá     0.9783    0.5056    0.6667        89\n",
            "        Xã hội     0.9865    0.7857    0.8748       280\n",
            "      Đời sống     0.9844    0.9464    0.9650       466\n",
            "\n",
            "     micro avg     0.9908    0.8518    0.9160      2894\n",
            "     macro avg     0.9059    0.7274    0.7985      2894\n",
            "  weighted avg     0.9756    0.8518    0.9056      2894\n",
            "   samples avg     0.9701    0.8835    0.9121      2894\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "topic_dataset = TextDataset(texts, topic_labels, tokenizer, is_multilabel=True)\n",
        "topic_loader = DataLoader(topic_dataset, batch_size=16, shuffle=True)\n",
        "model_topic = PhoBERT_Classifier(num_labels=len(topic_names), is_multilabel=True).to(\"cuda\")\n",
        "\n",
        "# Huấn luyện\n",
        "optimizer = torch.optim.AdamW(model_topic.parameters(), lr=2e-5)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "best_loss, patience = float(\"inf\"), 0\n",
        "\n",
        "for epoch in range(20):\n",
        "    model_topic.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in topic_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        labels = batch[\"label\"].to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model_topic(input_ids, attention_mask)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Dự đoán xác suất, sau đó chuyển thành nhị phân\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels.detach().cpu().numpy())\n",
        "\n",
        "    avg = total_loss / len(topic_loader)\n",
        "    y_true = np.vstack(all_labels)\n",
        "    y_pred = np.vstack(all_preds)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    print(f\"[Topic Epoch {epoch+1}] Loss: {avg:.4f} | Acc: {acc:.4f} | F1-macro: {f1_macro:.4f} | F1-micro: {f1_micro:.4f}\")\n",
        "\n",
        "    if avg < best_loss:\n",
        "        best_loss = avg\n",
        "        best_topic = copy.deepcopy(model_topic.state_dict())\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= 3:\n",
        "            break\n",
        "\n",
        "model_topic.load_state_dict(best_topic)\n",
        "print(classification_report(y_true, y_pred, target_names=topic_names, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fylhy2zuvHK4"
      },
      "source": [
        " 8. Dự đoán test + Xuất file CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EQgNNQ88vJJy",
        "outputId": "f1c7c9e9-f8b1-43af-f392-f6b01f8e6b8a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_cfb53db9-059e-4d5a-8ea0-6e498d295688\", \"submission.csv\", 12097)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_sent.eval()\n",
        "model_topic.eval()\n",
        "\n",
        "# Sentiment\n",
        "sent_preds = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_texts), 16):\n",
        "        enc = tokenizer(test_texts[i:i+16], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "        input_ids = enc[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = enc[\"attention_mask\"].to(\"cuda\")\n",
        "        logits = model_sent(input_ids, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        sent_preds.extend(preds)\n",
        "\n",
        "pred_sentiments = le_sent.inverse_transform(sent_preds)\n",
        "\n",
        "# Topic\n",
        "topic_preds = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_texts), 16):\n",
        "        enc = tokenizer(test_texts[i:i+16], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "        input_ids = enc[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = enc[\"attention_mask\"].to(\"cuda\")\n",
        "        logits = model_topic(input_ids, attention_mask)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        labels = (probs > 0.4).astype(int)\n",
        "        for row in labels:\n",
        "            selected_topics = [topic_names[i] for i in range(len(topic_names)) if row[i] == 1]\n",
        "            if selected_topics:\n",
        "              topics_str = \";\".join(selected_topics)\n",
        "            else:\n",
        "              topics_str = \"unknown\"\n",
        "            topic_preds.append(topics_str)\n",
        "\n",
        "\n",
        "# Gộp và xuất\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_df[\"id\"],\n",
        "    \"sentiment\": pred_sentiments,\n",
        "    \"topic\": topic_preds\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"submission.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
